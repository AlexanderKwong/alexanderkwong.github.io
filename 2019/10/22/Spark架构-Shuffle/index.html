<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Spark架构:Shuffle"><meta name="keywords" content="翻译,spark"><meta name="author" content="AlexanderKwong"><meta name="copyright" content="AlexanderKwong"><title>Spark架构:Shuffle | AlexanderKwong</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark架构-Shuffle"><span class="toc-number">1.</span> <span class="toc-text">Spark架构:Shuffle</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hash-Shuffle"><span class="toc-number">1.1.</span> <span class="toc-text">Hash Shuffle</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sort-Shuffle"><span class="toc-number">1.2.</span> <span class="toc-text">Sort Shuffle</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Unsafe-Shuffle-or-Tungsten-Sort"><span class="toc-number">1.3.</span> <span class="toc-text">Unsafe Shuffle or Tungsten Sort</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">AlexanderKwong</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">2</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">3</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">2</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://api.neweb.top/bing.php)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">AlexanderKwong</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">Spark架构:Shuffle</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-10-22</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/spark/">spark</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Spark架构-Shuffle"><a href="#Spark架构-Shuffle" class="headerlink" title="Spark架构:Shuffle"></a>Spark架构:Shuffle</h1><p>这是我的第二篇关于spark架构的文章，今天我将更详细地告诉你关于shuffle，在spark设计里面最有趣的话题之一。之前部分主要是关于spark大体的结构以及它的内存管理。可以通过<a href="https://0x0fff.com/spark-architecture/" target="_blank" rel="noopener">这里</a>访问。下一篇关于spark内存管理的，可以通过<a href="https://0x0fff.com/spark-memory-management/" target="_blank" rel="noopener">这里</a>访问。 </p>
<img src="/2019/10/22/Spark%E6%9E%B6%E6%9E%84-Shuffle/spark-shuffle.png" class="" title="spark-shuffle">

<p>总的来说shuffle是什么呢？想像一下你有一系列的通话记录在一个表中，你想每天计算通话的数量。你可以通过设置“天”作为你的键，然后每个记录你将之映射为数值“1”。然后你就可以为每个键将值加起来，那就是问题的答案——每天的总的记录数。但当你将数据存在集群的时候，怎么做才能将存储在不同机器上的对应着同一个键的值加起来呢？唯一可以做到的就是，为所有在同一个机器上拥有相同键的合一个值，然后你可将之加起来。 </p>
<p>有很多不同的任务要求数据在集群中shuffle，例如表关联——用字段id去关联两个表，你必要确保两个表中有着相同id值的所有数据都是存储在相同的块（chunk）中。假设这些表的键id都是integer类型，范围是1到1000000的值。通过存储数据在同一个块中，比如两个表中这个键的值在1到100的都存在一个块（partition/chunk），用上述方式而不是说，遍历第二张表找出对应第一张表的每一个parition，我们就能直接将partition和partition关联，因为我们知道键为1-100的值都只在这两个partition中。要实现两个表都有相同个数的partition，这种方式实现关联要求更少的计算量。所以现在你能明白shuffle有多重要了。 </p>
<p>要讨论这个话题，我将会按照MapReduce的命名规则。在shuffle操作中，在源执行器（source executor）中映射出数据的是mapper任务，将数据消费到目的执行器（target executor）的是reducer任务，两者之间就是shuffle。 </p>
<p>一般来说，shuffle有两个重要的压缩参数：<code>spark.shuffle.compress</code>——引擎是否压缩shuffle的输出；<code>spark.shuffle.spill.compress</code>——是否压缩shuffle中间数据的溢写文件。默认值都为true，都使用<code>spark.io.compression.codec</code>指定的codec来压缩数据，默认是snappy。 </p>
<p>你可能也知道，在spark中有很多shuffle的实现。哪一个实现会用你的场景中，是由<code>spark.shuffle.manager</code>参数所决定的。三个可选的参数是：hash，sort，tungsten-sort，“sort”选项是从spark1.2.0起默认的值。 </p>
<h2 id="Hash-Shuffle"><a href="#Hash-Shuffle" class="headerlink" title="Hash Shuffle"></a>Hash Shuffle</h2><p>在Spark1.2.0之前，只是shuffle选项的默认值（<code>spark.shuffle.manager=hash</code>）。但它有很多缺点，主要是由它<a href="http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project16_report.pdf" target="_blank" rel="noopener">创建的文件数</a>造成的——每个mapper任务为每个的reducer创建一个单独的文件，导致M * R个总文件数存于集群中，其中M是mapper个数，R是reuduce个数。当mapper和reducer数量很大时将造成很大的问题，包括输出缓冲区大小，文件系统中正被打开的文件数，创建和删除这些文件的速度。这有一个很好的<a href="http://spark-summit.org/2013/wp-content/uploads/2013/10/Li-AEX-Spark-yahoo.pdf" target="_blank" rel="noopener">例子</a>说明雅虎怎样面对这些问题，他们集群有46K个mapper和46K个reducer共在集群中产生了20亿个文件。 </p>
<p>这种shuffler的逻辑真是十分无语：它计算reducer的数量，这个数量作为reduce端的partition数量，为每一个都创建一个单独的文件，循环每一条要输出的记录，为每一条计算它的目标partition以及输出这条记录到相应的文件。 </p>
<p>看起来是这样的： </p>
<img src="/2019/10/22/Spark%E6%9E%B6%E6%9E%84-Shuffle/hash-shuffle.png" class="" title="hash-shuffle">

<p>对于这种shuffler有一个优化的实现，由参数<code>spark.shuffle.consolidateFiles</code>控制（默认是false）。当被设成true时，mapper输出文件会被合并。如果你的集群中由E个executor（对yarn来说就是“<code>--num-executors</code>”），它们的每个都有C个core（对yarn来说就是“<code>spark.executor.cores</code>”或“<code>--executor-cores</code>”），每个task需要T个CPU（“<code>spark.task.cpus</code>”），那么集群中的执行槽数量会有E * C / T 个，shuffle期间创建的文件数就等于 E * C / T * R。有100个executor10个core的话，每个task分配一个core，那么46000个reducer的情况下，能从2亿个文件下降到4600万个文件，就性能而言就好了很多。这个特性以一个<a href="https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/shuffle/FileShuffleBlockResolver.scala" target="_blank" rel="noopener">更直接简洁的方式</a>被实现了：代替为每个reducer创建新文件的是，创建一个输出文件的池。当map任务开始输出数据，它从池中请求数量为R个的一组文件。当它完成了，它归还这R个文件组到池中。由于每个executor只能并行地执行C / T个task，它会仅创建C / T个输出文件组，每组为R个文件。在第一个 C / T个并行任务完成之后，下一个 map任务会重用池中一个存在的文件组。 </p>
<p>这是总体上描述它怎样工作的图表： </p>
<img src="/2019/10/22/Spark%E6%9E%B6%E6%9E%84-Shuffle/hash-shuffle2.png" class="" title="hash-shuffle-consolidate">

<p>优点: </p>
<ol>
<li><p>快——完全没有要求排序，没有维持任务哈希表。 </p>
</li>
<li><p>数据排序没有内存开销。</p>
</li>
<li><p>没有多余IO开销——数据仅被写进硬盘一次和仅被读取一次。 </p>
</li>
</ol>
<p>缺点: </p>
<ol>
<li><p>当partition数量很大时，由于大量的输出文件，性能开始降低。 </p>
</li>
<li><p>对随机IO来说 ，大量的文件写到文件系统导致IO倾斜，一般来说要比顺序IO要慢100倍以上。 </p>
</li>
</ol>
<p>这里只给个引用地方，<a href="http://events.linuxfoundation.org/slides/2010/linuxcon2010_wheeler.pdf" target="_blank" rel="noopener">百万级文件规模</a>下，单个文件系统的IO操作缓慢的例子。 </p>
<p>当然了，当数据被写入文件时，它是序列化的和可选压缩的。当被读取的时候，处理顺序的反过来的——数据被解压和反序列化。Fetch端重要的参数是“<code>spark.reducer.maxSizeInFlight</code>”（默认是48MB），这个参数决定了每个远端的reducer请求数据的大小。来自不同executor的5个并行请求平均分配这个大小的数据，以加快处理速度。如果你增加了这个文件大小，你的reducer将从map任务输出中更大块地请求数据，会提升性能，但也增加reducer进程的内存消耗。 </p>
<p>如果reduce端的记录排序不是强制的话，那么reducer将仅仅返回一个迭代器，它持有map输出的依赖，但如果排序是必要的话，它将取到所有的数据，并在reduce端用<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala" target="_blank" rel="noopener">ExternalSorter</a>来将数据排序。 </p>
<h2 id="Sort-Shuffle"><a href="#Sort-Shuffle" class="headerlink" title="Sort Shuffle"></a>Sort Shuffle</h2><p>从spark1.2.0开始，这就被使用为默认的shuffle算法（<code>spark.shuffle.manager=sort</code>）。总的来说，这是企图去像<a href="https://0x0fff.com/hadoop-mapreduce-comprehensive-description/" target="_blank" rel="noopener">Hadoop MapReduce</a>那样子实现shuffle逻辑。在hash shuffle下你为每一个reducer输出一个单独的文件，当使用sort shuffle时你正在做这么一个机智的事情： 你输出一个按reducer的id排序并以id作为索引的文件，这种方式通过获取数据块在文件中的位置信息，在文件读取前的查找该位置，就能轻松地抓取与“reducer x”相关的数据块。但是，当然，对于小数量的reducer来说，显然通过散列来做文件分片比排序要快，因此sort shuffle有“候选”计划：当reducer数量少于<code>spark.shuffle.sort.bypassMergeThreshold</code>（默认200）我们就使用“后备”计划：把数据散列到独立文件中，然后把这些文件合并到一个文件中。这逻辑被单独实现在<a href="https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java" target="_blank" rel="noopener">BypassMergeSortShuffleWriter</a>类中。 </p>
<p>这个实现有趣的地方在于，在map端进行数据排序，但不会在reduce端合并排序的结果——万一确实需要数据排序时它才进行排序。Cloudera利用这个想法做着有意思的事情： <a href="http://blog.cloudera.com/blog/2015/01/improving-sort-performance-in-apache-spark-its-a-double/" target="_blank" rel="noopener">http://blog.cloudera.com/blog/2015/01/improving-sort-performance-in-apache-spark-its-a-double/</a> 。他们打开一个实现了这个逻辑的进程，利用预排序mapper输出的优势，来在reduce端合并数据而不是重排序。你可能知道，spark中reduce内排序使用<a href="https://en.wikipedia.org/wiki/Timsort" target="_blank" rel="noopener">TimSort</a>来完成，这是一个绝妙的排序算法，实际上它也是利用对输入进行预排序（通过计算minruns（TimSort的指标）然后合并在一起）。很多数学在这，你想的话也能跳过这部分。它用到的最小堆（Min Heap）算法， 是最高效的方式来实现，每合并M个含N个元素的数组的复杂度是O（MNlogM）。在TimSort中，我们创造了一个从数据找出MinRuns的途径，然后一对对地（pair-by-pair）合并它们。很显然，会确定出M个MinRuns。前M/2个合并会引起M/2个有序组，接下来M/4个合并会得出M/4个有序组如此类推，所以它很直接得出最后所有的这些会是O（MNlogM）的复杂度。跟直接合并有着相同的复杂度。区别就是常量，而常量依赖于具体实现。所以Cloudera工程师提供的<a href="https://issues.apache.org/jira/browse/SPARK-2926" target="_blank" rel="noopener">补丁</a>早已等待认可一年了，没有CM（Cloudera Managerment）的推动不太可能会被认可，因为性能上的提升是很少甚至几乎没有，你能够在JIRA的讨论标签下看到相关内容。可能他们会通过介绍新的单独的shuffle实现而不是“更新/提升”现有的主类来解决这个问题，我们拭目以待。 </p>
<p>这下好了。要是你没有足够内存来存储整个map的输出怎么办？你可能要将中间数据溢写硬盘。参数<code>spark.shuffle.spill</code>负责启用/禁用溢写，默认溢写功能是启动的。如果你禁用了它而又没有内存去保存map输出的话，你会直接得到OOM报错，所以要注意了。 </p>
<p>在溢写硬盘之前，能被用来存储map输出的内存数量等于 “jvm堆内存” * <code>spark.shuffle.memoryFration</code> * <code>spark.shuffle.safetyFraction</code>，默认是 “jvm堆内存” * 0.2 * 0.8 = “jvm堆内存” * 0.16。注意，如果你在同一个executor中运行多线程（要设置<code>spark.executor.cores</code> / <code>spark.task.cpus</code> 的比值大于1），用来存map输出的平均可用内存 就会是 “jvm堆内存” * <code>spark.shuffle.memoryFraction</code> * <code>spark.shuffle.safatyFracion</code> / <code>spark.executor.cores</code> * <code>spark.task.cpus</code>，其他参数默认情况下两个cpu的话即要有0.08 * “jvm堆内存”那么多。 </p>
<p>spark内部使用<a href="https://github.com/apache/spark/blob/branch-1.5/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala" target="_blank" rel="noopener">AppendOnlyMap</a>数据结构来存储map输出数据在内存中。因吹斯汀的是，spark使用它自己scala实现的散列表（hash table），这个实现用了“拉链法”（open hashing）以及用到二次探查（<a href="http://en.wikipedia.org/wiki/Quadratic_probing" target="_blank" rel="noopener">guadratic probing</a>）的同一个数据组存放键值。他们用到了Google Guava库的 murmur3_32作为散列函数，就是这个<a href="https://en.wikipedia.org/wiki/MurmurHash" target="_blank" rel="noopener">MurmurHash3</a>。 </p>
<p>spark应用combiner逻辑于这个散列表显得恰到好处——对已经存在的键添加每个新的值会与存在的值进行合并，合并的输出作为新的值。 </p>
<p>当发生溢写时，就在这个<a href="https://github.com/apache/spark/blob/branch-1.5/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala" target="_blank" rel="noopener">AppendOnlyMap</a>存储的数据之上调用“sorter”,sorter在这基础上做TimSort，然后这数据就写到硬盘上。 </p>
<p>当溢写发生或者已经没有mapper输出时，排好序的输出就会被写到硬盘，保证数据保存到硬盘上。是否真的保存到硬盘取决于操作系统的配置，比如缓冲缓存等，但都由操作系统来决定的，spark只是发送“写”命令而已。 </p>
<p>每一个被单独地溢写到硬盘的文件，它们仅在数据被reducer请求的时候才执行合并，并且合并是实时的，即，不会像在<a href="https://0x0fff.com/hadoop-mapreduce-comprehensive-description/" target="_blank" rel="noopener">Hadoop MapReduce</a>那样调用什么“硬盘合并”（on-disk-merger），仅通过从大量的单独的溢写文件中动态收集数据，然后用java里面PriorityQueue类所实现的<a href="https://en.wikipedia.org/wiki/Binary_heap" target="_blank" rel="noopener">Min Heap</a>将它们合并起来。 </p>
<p>工作原理： </p>
<img src="/2019/10/22/Spark%E6%9E%B6%E6%9E%84-Shuffle/sort-shuffle.png" class="" title="sort-shuffle">

<p>因此这种shuffle： </p>
<p>优点： </p>
<p>   1、map端创建更少的文件。 </p>
<p>   2、更少的随机IO操作，大部分顺序读写。 </p>
<p>缺点： </p>
<p>   1、排序比哈希（散列）要慢。为你的集群找到最佳的点，通过调整bypassMergeThreshold这个参数来实现，这是值得的，但通常来说，对于大部分集群，甚至默认值就太高了。 </p>
<p>   2、万一你使用SSD硬盘来放spark shuffle的临时数据，hash shuffle可能更好。 </p>
<h2 id="Unsafe-Shuffle-or-Tungsten-Sort"><a href="#Unsafe-Shuffle-or-Tungsten-Sort" class="headerlink" title="Unsafe Shuffle or Tungsten Sort"></a>Unsafe Shuffle or Tungsten Sort</h2><p>在spark1.4.0+，可以通过设置<code>spark.shuffle.manager=tungsten-sort</code>来启用。这是”<a href="https://issues.apache.org/jira/browse/SPARK-7075" target="_blank" rel="noopener">Tungsten</a>“工程代码的一部分。<a href="https://issues.apache.org/jira/browse/SPARK-7081" target="_blank" rel="noopener">这里</a>描述了它的思路，相当有意思。这个shuffle实现的优化有： </p>
<ol>
<li><p>不需要反序列化，直接操作序列化后的二进制数据。用到unsafe（<code>sun.misc.Unsafe</code>）内存复制函数来复制数据自身，对于序列化数据来说行之有效，由于事实上序列化数据就是一个字节数组。 </p>
</li>
<li><p>使用专门的高效缓存的sorter，<a href="https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java" target="_blank" rel="noopener">ShuffleExternalSorter</a>来对压缩的记录的指针和parition的id进行排序。在数组排序的过程中，每秒仅使用8字节的空间，使用CPU缓存更加高效。 </p>
</li>
<li><p>由于记录没有被反序列化，可以直接执行溢写序列化数据（而非 反序列化-比较-序列化-溢写 这样的逻辑）。 </p>
</li>
<li><p>当shuffle压缩的编解码器（<code>shuffle compression codec</code>）支持序列化流的串连（就是通过连结文件来合并各个溢写输出）时，额外的 溢写-合并 优化自动被应用，当前spark的LZF序列化器支持以上优化，仅当快速合并通过参数<code>shuffle.unsafe.fastMergeEnabled</code>来启用。 </p>
</li>
</ol>
<p>作为下一步的优化，这个算法也会被介绍，堆外存储缓冲（<a href="https://issues.apache.org/jira/browse/SPARK-7542" target="_blank" rel="noopener">off-heap storage buffer</a>）。 </p>
<p>这个shuffle实现仅仅用于同时出现以下情况： </p>
<ul>
<li><p>shuffle的依赖没有指定聚合。使用聚合意味着需要存储反序列化的值来聚合成新的值。这种情况下你失去了这种shuffle的主要优势，因为用到了序列化数据的操作。 </p>
</li>
<li><p>shuffle序列化器支持重分配序列化后的值（当前是Kryo序列化器所支持的，以及spark sql自定义序列化器）。 </p>
</li>
<li><p>shuffle输出少于16777216个partition </p>
</li>
<li><p>没有单独的序列化形式的记录是大于128MB </p>
</li>
</ul>
<p>你也必须明白，以这种shuffle进行的排序仅通过partition id被执行，就是说不再可能用TimSort在reduce端 合并已预排序的数据。这个操作中的排序是基于8字节的值，每个值都对链接到序列化数据项和分区号，这就是为什么会有16亿输出分区数的限制的原因。 </p>
<p>工作原理： </p>
<img src="/2019/10/22/Spark%E6%9E%B6%E6%9E%84-Shuffle/Tungsten-sort.png" class="" title="Tungsten-sort">

<p>首先，对于每个数据溢写文件，对描述它的指针数组以及输出一个有索引的分区文件进行排序，然后合并这些分区文件进一个单独的有索引的输出文件。 </p>
<p>优点: </p>
<ol>
<li>上述诸多性能优化。 </li>
</ol>
<p>缺点: </p>
<ol>
<li><p>没有处理mapper端数据排序问题。 </p>
</li>
<li><p>没有提供堆外排序缓冲区。 </p>
</li>
<li><p>不稳定。</p>
</li>
</ol>
<p>但我的看法是这个排序是spark设计中一个很大的优点，我想看看这将会有怎样的结果，以及Databricks团队会有哪些新的性能benchmarks，由这些新特性可展示给我们多么多么cool的性能。 </p>
<p>这就是全部我想说的关于spark shuffles。一块很有趣的代码，如果你有时间我建议你自己去阅读它。 </p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">AlexanderKwong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://alexanderkwong.github.io/2019/10/22/Spark%E6%9E%B6%E6%9E%84-Shuffle/">https://alexanderkwong.github.io/2019/10/22/Spark%E6%9E%B6%E6%9E%84-Shuffle/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%BF%BB%E8%AF%91/">翻译</a><a class="post-meta__tags" href="/tags/spark/">spark</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/2019/10/19/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E2%80%94%E2%80%94Hadoop-MapReduce%E5%85%A8%E6%96%B9%E4%BD%8D%E6%8F%8F%E8%BF%B0/"><span>分布式系统架构——Hadoop MapReduce全方位描述</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://api.neweb.top/bing.php)"><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By AlexanderKwong</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>