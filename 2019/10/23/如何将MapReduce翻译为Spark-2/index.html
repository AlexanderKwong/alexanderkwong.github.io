<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="如何将MapReduce翻译为Spark(2)"><meta name="keywords" content="翻译,spark,mapreduce"><meta name="author" content="AlexanderKwong"><meta name="copyright" content="AlexanderKwong"><title>如何将MapReduce翻译为Spark(2) | AlexanderKwong</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#如何将MapReduce翻译为Spark-2"><span class="toc-number">1.</span> <span class="toc-text">如何将MapReduce翻译为Spark(2)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#系列文章的总结-包括Combiner-like的聚合功能，计数器，分区以及序列化"><span class="toc-number">1.1.</span> <span class="toc-text">系列文章的总结 包括Combiner-like的聚合功能，计数器，分区以及序列化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reduceByKey-vs-Combiner"><span class="toc-number">1.2.</span> <span class="toc-text">reduceByKey vs Combiner</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据分区和分组"><span class="toc-number">1.3.</span> <span class="toc-text">数据分区和分组</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计数器"><span class="toc-number">1.4.</span> <span class="toc-text">计数器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#关于使用累加器重要的警告"><span class="toc-number">1.5.</span> <span class="toc-text">关于使用累加器重要的警告</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#序列化框架"><span class="toc-number">1.6.</span> <span class="toc-text">序列化框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#结论"><span class="toc-number">1.7.</span> <span class="toc-text">结论</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">AlexanderKwong</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">6</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">3</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">2</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://api.neweb.top/bing.php)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">AlexanderKwong</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">如何将MapReduce翻译为Spark(2)</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-10-23</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="如何将MapReduce翻译为Spark-2"><a href="#如何将MapReduce翻译为Spark-2" class="headerlink" title="如何将MapReduce翻译为Spark(2)"></a>如何将MapReduce翻译为Spark(2)</h1><h2 id="系列文章的总结-包括Combiner-like的聚合功能，计数器，分区以及序列化"><a href="#系列文章的总结-包括Combiner-like的聚合功能，计数器，分区以及序列化" class="headerlink" title="系列文章的总结 包括Combiner-like的聚合功能，计数器，分区以及序列化"></a>系列文章的总结 包括Combiner-like的聚合功能，计数器，分区以及序列化</h2><p>Apache Spark逐渐被普及为MapReduce的一个替代品，很大一部分原因由于它在数据处理中形象的API。过去几个月，我的同事Sean Owen写了一片文章将MapReduce的功能翻译为Spark，而这篇文章，我将延续那个主题，来讲述另外的功能。 </p>
<p>简短地回顾一下，MapReduce原本设计来做ETL操作以及大量日志处理。MapReduce依靠在map和reduce阶段处理key-value对。每一阶段有以下动作： </p>
<ol>
<li>Map：对每一个输入，吐出0个、1个或更多的key-value对作为输出。 </li>
<li>Shuffle：根据相同Key，通过集群网络上shuffle数据来将key-value对分组。 </li>
<li>Reduce：操作每个key相关的values的迭代器，通常执行一些聚合。 </li>
</ol>
<p>为了执行复杂的操作，很多Map和Reduce阶段必须捆绑在一块。由于MapReduce变得更加流行，它的复杂和重复操作各方面暴露出它的局限性。 </p>
<p>Spark提供围绕弹性分布式数据集的一套处理API。你可以通过读文件创建一个RDD，然后对RDD指定一系列你想要的算子，比如解析记录，按key分组，相关的value求均值等。Spark允许你对RDD指定两种不同类型的算子：转换(transformation)和行动(action)。转换算子说的是怎样将一个数据集合转化成另一个。转换算子的例子有map,flatMap和groupByKey。行动算子请求执行计算，比如将输出写到一个文件或者在屏幕上打印一个变量等。 </p>
<p>Spark使用惰性计算模型，意味着直到调用一个行动算子，计算才真正被触发。对RDD调用一个行动算子触发所有的必要的转换算子的执行。这种惰性求值让Spark能够智能的合并算子以及优化性能。 </p>
<p>在博文的剩余部分，我们将探究，Spark集群成功部署的情况下，如何在Spark中再实现MapReduce下你可能已经熟悉的功能。尤其是，我们将涵盖combiner-like的聚合功能，数据分区(parition)，counter-like功能，以及相关的插件式的序列化框架。 </p>
<h2 id="reduceByKey-vs-Combiner"><a href="#reduceByKey-vs-Combiner" class="headerlink" title="reduceByKey vs Combiner"></a>reduceByKey vs Combiner</h2><p>这个简单Mapper在上一篇博文用到： </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LineLengthMapper</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Mapper</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable lineNumber, Text line, Context context)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    context.write(<span class="keyword">new</span> IntWritable(line.getLength()), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 这是一个按行长度统计行数的Job的一部分。很简单，但是效率低：Mapper为每一行写长度和计数1，然后会写到磁盘上并通过网络进行shuffle，在Reducer上只做了求和。假如有一百万个空行，就会产生一百万条“length 0,count 1”的记录通过网络来拷贝，仅仅被一个像上次提及的Reducer给浓缩(collapse)为“length 0,count 1000000”：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LineLengthReducer</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Reducer</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable length, Iterable counts, Context context)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (IntWritable count : counts) &#123;</span><br><span class="line">      sum += count.get();</span><br><span class="line">    &#125;</span><br><span class="line">    context.write(length, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>于是，MapReduce有了Combiner的概念。一个Combiner是一次优化，它像Reducer一样工作——事实上，它必须是Reducer的一个实现——能够在Mapper端在记录被写出来之前合并多个记录。它的功能像一个小型的Reducer以减少shuffle。Combiner必须满足交换率和结合率，就是说必须无论它所合并的记录是什么顺序，结果都是一样的。事实上，LineLengthReducer自身可以作为MapReduce中的Combiner，像Reducer一样的。 </p>
<p>回到Spark。对LineLengthMapper和LineLengthReducer的一个简洁的字面上的翻译如下，当然这不是最佳的： </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linesRDD.mapPartitions &#123; lines =&gt;</span><br><span class="line">lines.map(line =&gt; (line.length, <span class="number">1</span>))</span><br><span class="line">&#125;.groupByKey().mapValues(_.sum)</span><br></pre></td></tr></table></figure>

<p> Mapper对应的调用是mapPartitions，shuffle对应groupByKey，以及Reducer对应mapValues。同样的，Combiner字面上的翻译，会在Mapper模拟的结束注入它自己的逻辑，mapPartitions：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">linesRDD.mapPartitions &#123; lines =&gt;</span><br><span class="line">  <span class="keyword">val</span> mapResult = lines.map(line =&gt; (line.length, <span class="number">1</span>))</span><br><span class="line">  mapResult.toSeq.groupBy(_._1).mapValues(_.map(_._2).sum).toIterator</span><br><span class="line">&#125;.groupByKey().mapValues(_.sum)</span><br></pre></td></tr></table></figure>

<p>新代码使用Scala的集合API，这些并不是Spark的算子。正如之前所说的，新代码实际上正是这个逻辑。由于Spark中模仿了许多Scala的API，很容易可以看到这两个版本中所相似的表达。 </p>
<p>这仍有不好的影响。这个算子的本质是将计数求和，要知道怎样将所有计数求和只需要知道怎样去求和两个数，然后反复应用其上直到只剩下一个值。这就是一个reduce该干的事情：在一个函数中将两个值合为一个，终将使很多值变为一个。 </p>
<p>事实上，如果仅给出reduce函数，Spark可以智能地应用它，故而一次实现上述Combiner和Reducer的的所有效果： </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linesRDD.mapPartitions &#123; lines =&gt;</span><br><span class="line">  <span class="keyword">val</span> mapResult = lines.map(line =&gt; (line.length, <span class="number">1</span>))</span><br><span class="line">&#125;.reduceByKey(_ + _)</span><br></pre></td></tr></table></figure>

<p> <em>+</em>是代表着返回两个参数求和的函数的速写。这是在Spark的这个算子中的一个更通用的写法，而在底层，Spark在shuffle前后自动地应用这个reduce函数。实际上，没有在代码中编写Combiner相应代码的需要的话，就不必再用mapPartitions来对一个分区键值对编写map方法，因为可以写成一次map一个元素如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linesRDD.map(line =&gt; (line.length, <span class="number">1</span>)).reduceByKey(_ + _)</span><br></pre></td></tr></table></figure>

<p>结果就是，当使用Spark时，你们自然地用上了Combiner的等价物了。感兴趣的可以看看，如下是一些更深入的说明： </p>
<ul>
<li>reduceByKey是在一个更为通用的Spark算子上建立的，它叫combineByKey，它能使values在同一时间被转换。 </li>
<li>对于那些实际上在为values计数的，Spark中有一个更为直接的方法：linesRDD.map(_.length).countByValue() </li>
<li>如果速度比精确度更重要的话，有一个更快的近似版本，它依赖HyperLogLog算法：linesRDD.map(_.length).countByValueApprox() </li>
</ul>
<h2 id="数据分区和分组"><a href="#数据分区和分组" class="headerlink" title="数据分区和分组"></a>数据分区和分组</h2><p>Spark和MapReduce都支持将key-value数据按key来分区。处理框架如何将数据分片并轮流执行tasks，这对通用的数据算子的性能有巨大的影响，比如关联不同的数据集 或者 按key聚合 等。 </p>
<p>在MapReduce中，你可以指定一个paritioner来决定key-value对按怎样的规则分开并分配到reudcer去。一个拥有好的设计的partitionre会近似均匀地将记录分配到reducer。MapReduce和Spark都使用hash分区作为它们默认的分区策略，尽管两者的实现不同。Hash分区依赖于将key求hash值来分发key-value对。在MapReduce和Spark中一个key-value对被分发到的分区， 是由 hashCode()方法返回值对 你将创建的分区数 求模运算 后的值决定。期望是hash函数将在hash-space中均匀的分配你的key，最终得到近似将数据均匀地分到parition中。 </p>
<p>分布式程序中按key聚合的常见问题是分到reducer的记录数存在“长尾效应”，存在“艰难的”reudcer花费比其它reudcer更多的时间去完成任务。通常你可以通过指定一个不同的、很可能量身定制的partitioner来解决这个问题。在MapReduce中，你可以通过继承Partitioner自定义你定制的分区类然后在job的配置中指定。既能通过配置文件来实现，也可以程序中指定conf.setPartitionerClass(MyPartitioner.class)。 </p>
<p>在Spark中，有些算子能从分区中受益，还有些算子能修改分区。下面表格解释什么类型转换能影响分区以及如何影响的： </p>
<img src="/2019/10/23/%E5%A6%82%E4%BD%95%E5%B0%86MapReduce%E7%BF%BB%E8%AF%91%E4%B8%BASpark-2/api.png" class="">

<h2 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h2><p>MapReduce允许你在job中为某事情出现次数进行统计，稍后查询该统计值。在MapReduce中定义计数器，首先需要定一个Enum来描述你讲追踪的这些计数器。假设你通过一个jackson ObjectMapper利用Jackson(org.codehaus.jackson)来解析JSON为POJO对象，此过程中，你可能遇到一个JsonParseException或JsonMappiingException，然后你可能想要跟踪有多少个这样的。你就得创建一个常量，它包含这所有这些异常： </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">enum</span> JsonErr &#123;</span><br><span class="line">      PARSE_ERROR,</span><br><span class="line">      MAPPING_ERROR</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 然后，在你的map类的map方法中，会有：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line"><span class="comment">/* Parse a string into a POJO */</span></span><br><span class="line">   &#125; <span class="keyword">catch</span> (JsonParseException parseException) &#123;</span><br><span class="line">context.getCounter(JsonErr.PARSE_ERROR).increment(<span class="number">1L</span>)</span><br><span class="line">   &#125; <span class="keyword">catch</span> (JsonMappingException mappingException) &#123;</span><br><span class="line">context.getCounter(JsonErr.MAPPING_ERROR).increment(<span class="number">1L</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>所有的计数器在一个job的过程中会汇报到JobTracker并展示到JobTracker的Web UI，跟默认I/O的计数器一样。你也可以从MapReduce的driver端，在你自己的配置所创建的Job中访问这些counter。 </p>
<p>Spark暴露了累加器，能像计数器一样的用，但更一般地支持任何组合算子（译者注：应指的不仅map/reduce其它的也都支持）。因此你不仅能通过整数来加减，甚至可以用浮点数，来收集你实际上所遇到的解析错误。 </p>
<p>如果你打算在Spark中字面上翻译这样的解析错误数，如： </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Define accumulators. */</span></span><br><span class="line"><span class="keyword">val</span> parseErrorAccum = sc.accumulator(<span class="number">0</span>, <span class="string">"JsonParseException"</span>)</span><br><span class="line"><span class="keyword">val</span> mappingErrorAccum = sc.accumulator(<span class="number">0</span>, <span class="string">"JsonMappingException"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">/** Define a function for parsing records and incrementing accumulators when exceptions are thrown. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span></span>(line: <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line"><span class="comment">/* Parse a String into a POJO */</span></span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">JsonParseException</span> =&gt; mapErr += <span class="number">1</span></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">JsonMappingException</span> =&gt; parseError += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/** Parse records in a transformation.*/</span></span><br><span class="line"><span class="keyword">val</span> parsedRdd = unparsedRdd.map(parse)</span><br></pre></td></tr></table></figure>

<p>然而目前没有一个好的方式在执行转换(transformation)时计数。Spark的累加器确实提供了有效的功能来创建解析错误的样本。很显然在MapReduce完成这个就难多了。一个替代的和有用的策略是，蓄水池取样算法(reservoir sampling)来创建一个解析错误的错误信息的样本 </p>
<h2 id="关于使用累加器重要的警告"><a href="#关于使用累加器重要的警告" class="headerlink" title="关于使用累加器重要的警告"></a>关于使用累加器重要的警告</h2><p>现在，你该关注怎样并何时使用累加器。在MapReduce中，累加器增加被触发后如若任务失败了计数不会累计到最终的值里。MapReduce很细心，即使任务失败或者发生推测执行时也会正确地计数。 </p>
<p>在Spark中，累加器的行为需要额外注意。强烈建议仅在行动算子中使用累加器。行动算子中对累加器增加会保证仅增加一次。如果一个task或job stage曾经重跑的话，在转换算子中对累加器增加可能增加多次，这对大部分用户来说都是不期待的行为。 </p>
<p>在下面的例子中，创建了一个RDD，然后map遍历时累加器自增。由于Spark使用一个惰性执行模型，这些RDD一次action只会计算一次并返回有且仅有一个值。在myRdd2上 调用另一个action要求工作流重新计算之前的步骤，再次增加累加器的值。 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">myrdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at :<span class="number">12</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> myaccum = sc.accumulator(<span class="number">0</span>, <span class="string">"accum"</span>)</span><br><span class="line">myaccum: org.apache.spark.<span class="type">Accumulator</span>[<span class="type">Int</span>] = <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> myRdd2 = myrdd.map(x =&gt; myaccum += <span class="number">1</span>)</span><br><span class="line">myRdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Unit</span>] = <span class="type">MappedRDD</span>[<span class="number">1</span>] at map at :<span class="number">16</span></span><br><span class="line"> </span><br><span class="line">scala&gt; myRdd2.collect</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Unit</span>] = <span class="type">Array</span>((), (), ())</span><br><span class="line"> </span><br><span class="line">scala&gt; myaccum</span><br><span class="line">res1: org.apache.spark.<span class="type">Accumulator</span>[<span class="type">Int</span>] = <span class="number">3</span></span><br><span class="line"> </span><br><span class="line">scala&gt; myRdd2.collect</span><br><span class="line">res2: <span class="type">Array</span>[<span class="type">Unit</span>] = <span class="type">Array</span>((), (), ())</span><br><span class="line"> </span><br><span class="line">scala&gt; myaccum</span><br><span class="line">res3: org.apache.spark.<span class="type">Accumulator</span>[<span class="type">Int</span>] = <span class="number">6</span></span><br></pre></td></tr></table></figure>

<p>语义上并非一次且仅有一次（once-and-only-once）模型。 </p>
<p>上面例子中统计解析错误次数的问题是，job可能在没有明确错误下成功执行，但结果的数值不一定有效，然而却很难发现。累加器在MapReduce中最通常的用法是在解析记录的同时统计错误数，很不幸的是，Spark中的累加器没有提供可靠的计数方式。 </p>
<h2 id="序列化框架"><a href="#序列化框架" class="headerlink" title="序列化框架"></a>序列化框架</h2><p>MapReduce和Spark都需要能够从JVM中获取对象并当shuffle数据时序列化它们为二进制字节码形式以在网络上传输。MapReduce使用一个可插拔的框架，允许用户来指定他们自已对org.apache.hadoop.io.serializer.Serialization的实现，通过在hadoop的配置中设置io.serialization使用他们自定义的序列化器。HadoopWritable和Avro以及基于反射的序列化器被配置为默认支持的序列化方式。 </p>
<p>类似地，Spark有一个可插拔的序列化系统，能通过在Spark的配置中设置spark.serializer变量来设定，要求继承org.apache.spark.serualizer.Serializer。Spark默认使用Java序列化机制，虽然方便快捷但比起其它序列化方式要更慢。通过设置spark.serializer为org.apache.spark.serializer.KryoSerializer，有需要的话设置spark.kryo.registrator为你自定义的注册器类，以使用更快的Kryo序列化协议。为了从Kryo中获取最佳性能，你应该首先注册一个KryoRegistrator类，并配置Spark为你专门的Kryo注册器。 </p>
<p>如果你渴求速度，想要使用Kryo来序列化，以及注册一个User类，你该想这样定义你的注册器： </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.mycompany.model.<span class="type">User</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.<span class="type">KryoRegistrator</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyKryoRegistrator</span> <span class="keyword">extends</span> <span class="title">KryoRegistrator</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerClasses</span></span>(kryo: <span class="type">Kryo</span>) &#123;</span><br><span class="line">    kryo.register(classOf[<span class="type">User</span>])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 你要设置spark.serializer为spark.KryoSerializer以及spark.kryo.registrator为com.mycompany.myproject.MyKryoRegistrator。不值得使用Avro对象，因为你还得去指定AvroSerializer类来序列化和反序列化。你可以像下面这样修改你的注册器代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.mycompany.model.<span class="type">UserAvroRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.<span class="type">KryoRegistrator</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyKryoRegistrator</span> <span class="keyword">extends</span> <span class="title">KryoRegistrator</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerClasses</span></span>(kryo: <span class="type">Kryo</span>) &#123;</span><br><span class="line">    kryo.register(classOf[<span class="type">UserAvroRecord</span>], <span class="keyword">new</span> <span class="type">AvroSerializer</span>[<span class="type">UserAvroRecord</span>]())</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>敲黑板：当数据用Spark在网络传输的话，将使用你在配置中指定的序列化器来序列化，task闭包将使用Java序列化机制来序列化。这就意味着，你的task闭包中所有的东西必须是可序列化的(serializable)，否则你将得到TaskNotSerializableException。 </p>
<p>为了让Spark在你的RDD中操作数据，必须能够序列化你driver代码里map、flatMap、combineByKey中所指定的函数对象。这总是使用java序列化机制来完成，就是说你并不能简单地在Spark函数闭包中使用Avro对象，除非使用的是JDK1.8（Arvo在java8中实现了serializable）。 </p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>正如你观察到的，在coombiner-like聚合功能，分区，计数器，和插件式的序列化框架等方面，MapReduce和Spark有相似亦有区别之处。理解这些细微差别帮你确保你的Spark应用获得长期的成功。 </p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">AlexanderKwong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://alexanderkwong.github.io/2019/10/23/%E5%A6%82%E4%BD%95%E5%B0%86MapReduce%E7%BF%BB%E8%AF%91%E4%B8%BASpark-2/">https://alexanderkwong.github.io/2019/10/23/%E5%A6%82%E4%BD%95%E5%B0%86MapReduce%E7%BF%BB%E8%AF%91%E4%B8%BASpark-2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%BF%BB%E8%AF%91/">翻译</a><a class="post-meta__tags" href="/tags/spark/">spark</a><a class="post-meta__tags" href="/tags/mapreduce/">mapreduce</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/2019/10/23/%E5%A6%82%E4%BD%95%E5%B0%86MapReduce%E7%BF%BB%E8%AF%91%E4%B8%BASpark-1/"><span>如何将MapReduce翻译为Spark(1)</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://api.neweb.top/bing.php)"><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By AlexanderKwong</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>