<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="如何将MapReduce翻译为Spark(1)"><meta name="keywords" content="翻译,spark,mapreduce"><meta name="author" content="AlexanderKwong"><meta name="copyright" content="AlexanderKwong"><title>如何将MapReduce翻译为Spark(1) | AlexanderKwong</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#如何将MapReduce翻译为Spark-1"><span class="toc-number">1.</span> <span class="toc-text">如何将MapReduce翻译为Spark(1)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#要发挥Spark最大效用的关键是理解RDD-API和原始Mapper与Reducer-API之间的区别"><span class="toc-number">1.1.</span> <span class="toc-text">要发挥Spark最大效用的关键是理解RDD API和原始Mapper与Reducer API之间的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tuple形式的Key-Value对"><span class="toc-number">1.2.</span> <span class="toc-text">Tuple形式的Key-Value对</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reducer与reduce-对比-reduceByKey"><span class="toc-number">1.3.</span> <span class="toc-text">Reducer与reduce() 对比 reduceByKey()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mapper与map-对比-flatMap"><span class="toc-number">1.4.</span> <span class="toc-text">Mapper与map() 对比 flatMap()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#groupByKey"><span class="toc-number">1.5.</span> <span class="toc-text">groupByKey()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#setup-和cleanup"><span class="toc-number">1.6.</span> <span class="toc-text">setup()和cleanup()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#等一下，还有更多"><span class="toc-number">1.7.</span> <span class="toc-text">等一下，还有更多</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">AlexanderKwong</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">6</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">3</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">2</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://api.neweb.top/bing.php)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">AlexanderKwong</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">如何将MapReduce翻译为Spark(1)</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-10-23</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="如何将MapReduce翻译为Spark-1"><a href="#如何将MapReduce翻译为Spark-1" class="headerlink" title="如何将MapReduce翻译为Spark(1)"></a>如何将MapReduce翻译为Spark(1)</h1><h2 id="要发挥Spark最大效用的关键是理解RDD-API和原始Mapper与Reducer-API之间的区别"><a href="#要发挥Spark最大效用的关键是理解RDD-API和原始Mapper与Reducer-API之间的区别" class="headerlink" title="要发挥Spark最大效用的关键是理解RDD API和原始Mapper与Reducer API之间的区别"></a>要发挥Spark最大效用的关键是理解RDD API和原始Mapper与Reducer API之间的区别</h2><p>宝贵的MapReduce在项目初期就成为Apache Hadoop大规模计算的范式。它是原始设计的Hadoop处理各种工作的理想模型，如：大规模的日志处理，批量的面向ETL的操作等。 </p>
<p>由于Hadoop的广泛使用，很显然，对于所有的计算，MapReduce并不是最好的框架。Hadoop的将资源管理抽象出来作为它自己的第一类组件（YARN等）为可替代的架构创造了空间，比如Impala等项目已经能在平台上使用新的、专门的、非MapRdeuce架构来增加交互SQL。 </p>
<p>今天，Spark是另一个这样的可替代的框架以及被很多人说是接替MapReduce作为Hadoop通用的计算范式。但如果MapReduce已如此有成效，它又怎么会突然被替代？别忘了，仍有大量的ETL工作要在Hadoop上完成，即使这个平台现在也有了其它的实时的功能。 </p>
<p>庆幸的是，在Spark中实现MapReduce-like形式的计算是完全可能的。它们能简化维护以及某些场景下更快，这得益于Spark优化掉溢写磁盘。而对于MapReduce在Spark上重新实现就是家常便饭(homecoming)。Spark毕竟效仿了Scala的函数式编程风格和API。而恰恰MapReduce的想法是来自于函数式编程语言LISP。 </p>
<p>尽管Spark的主要抽象——RDD（弹性分布式数据集），原生地暴露的map()和reduce()算子，但这俩并不是直接模拟Hadoop的Mapper或Reducer的API的。这通常被开发者误以为是Mapper和Redeucer类在Spark中的等价物。 </p>
<p>对比在Scala或Spark中使用经典的函数式语言所实现的map()和reduce()，Hadoop中Mapper和Reducer的API事实上更加的灵活，故而更加的复杂。对习惯于MapReduce的开发者来说，这些区别可能还不是透明的，但是，相对于MapReduce这个抽象概念，下列行为是实实在在的Hadoop所实现中的细节： </p>
<ul>
<li>Mapper和Reducer总是使用key-value对作为输入输出。 </li>
<li>一个Reducer每次仅reduce一个key。（译者注：这里应该指默认情况下，可通过job.setGroupComparator()自定义） </li>
<li>对于每个输入，一个Mapper或Reducer可能吐出0个，1个或者更多的key-value对。 </li>
<li>Mapper和Reducer可能吐出任意的Keys或values，不仅仅是输入的子集或转换。 </li>
<li>Mapper和Reducer对象有在跨越多次的map()和reduce()调用的生命周期。并且支持setup()和cleanup()方法，用于在记录批量处理之前或之后执行。 </li>
</ul>
<p>这篇文章将简短地论证如何上述这些如何在Spark中一一实现，而且还会揭示：不必逐字逐句来翻译一个Mapper和Reducer！ </p>
<h2 id="Tuple形式的Key-Value对"><a href="#Tuple形式的Key-Value对" class="headerlink" title="Tuple形式的Key-Value对"></a>Tuple形式的Key-Value对</h2><p>现在我们想要计算一个大文本输入的每行的长度，然后通过每行长度来汇报行号。在Hadoop MapReduce中，先在Mapper中处理key-value对，以长度为Key行号为value： </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LineLengthMapper</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable lineNumber, Text line, Context context)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    context.write(<span class="keyword">new</span> IntWritable(line.getLength()), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Mapper和Reducer仅操作key-value对，那也没什么用。所以LineLengthMapper的输入得由TextInputFormat提供，事实上，value就是行的内容，key就是这行在文件中的相对位置，它是来搞笑的。（它很少用得上，但总得有东西作为key对吧） </p>
<p>Spark中等价于： </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lines.map(line =&gt; (line.length, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>在Spark中，输入只是一个String的RDD，而非key-value对。Spark代替key-value对的是一个Scala的tuple，由上面展示的（a，b）语法来创建。上面的map()算子的结果是一个形如tuple（int, int）的RDD。当一个RDD包含tuple，它会有更多的方法，比如reduceByKey()，这些方法本质上就再现了MapReduce的行为。 </p>
<h2 id="Reducer与reduce-对比-reduceByKey"><a href="#Reducer与reduce-对比-reduceByKey" class="headerlink" title="Reducer与reduce() 对比 reduceByKey()"></a>Reducer与reduce() 对比 reduceByKey()</h2><p>要产生行长度的统计，一定得每个长度的计数在同一个Reducer中求和： </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LineLengthReducer</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">IntWritable</span>,<span class="title">IntWritable</span>,<span class="title">IntWritable</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable length, Iterable&lt;IntWritable&gt; counts, Context context)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (IntWritable count : counts) &#123;</span><br><span class="line">      sum += count.get();</span><br><span class="line">    &#125;</span><br><span class="line">    context.write(length, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 上述Mapper和Reducer加起来等价于Spark的一行代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lengthCounts = lines.map(line =&gt; (line.length, <span class="number">1</span>)).reduceByKey(_ + _)</span><br></pre></td></tr></table></figure>

<p>Spark中RDD的API有一个reduce()方法，但它会将全部的key-value对都reduce成一个单独的值。这与Hadoop MapReduce干的不相符。反而，Reducer会将同一个key的value吐出为一个key以及相应的value。reduceByKey()是更相似的模拟。但那还不是Spark中最直接等价的：参见下面的groupByKey()。 </p>
<p>这里值得指出的是，一个Reducer的reduce()方法接收一个许多value的流，而产生0个、1个或者更多的结果。对比之下，reduceByKey()接收一个函数将两个value转换成有且仅有一个value，而在这个例子，一个简单的额外的函数将两个数字求和。这个组合函数调用能reduce多个value为一个。相比一个Reducer所暴露的API，就reducing values来说，这是一个更简单，更狭义的API。 </p>
<h2 id="Mapper与map-对比-flatMap"><a href="#Mapper与map-对比-flatMap" class="headerlink" title="Mapper与map() 对比 flatMap()"></a>Mapper与map() 对比 flatMap()</h2><p>现在，考虑所有单词转成大写后出现次数的统计。对于输入的每一行文本，一个Mapper可能吐出0个、1个或多个key-value对： </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountUppercaseMapper</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable lineNumber, Text line, Context context)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (String word : line.toString().split(<span class="string">" "</span>)) &#123;</span><br><span class="line">      <span class="keyword">if</span> (Character.isUpperCase(word.charAt(<span class="number">0</span>))) &#123;</span><br><span class="line">        context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> Spark中等价于：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lines.flatMap(</span><br><span class="line">  _.split(<span class="string">" "</span>).filter(word =&gt; <span class="type">Character</span>.isUpperCase(word(<span class="number">0</span>))).map(word =&gt; (word,<span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>map()将在这就不能满足了，因为map()对于每个输入必须产生有且仅有一个输出，但不像前面所说的一行可能需要产生多个输出。再者，对比Mapper的API所支持的 ，Spark中的map()函数更为简单和狭义。 </p>
<p>Spark中的解决方案是先将每一行映射(map)为一个输出值的数组。这个数组可能是空的也可能有多个值。仅将行映射成数组终将产生一个数组的RDD，这个RDD应该为那些数组的内容。然后，这个RDD需要“压扁”，flatMap()就是干这个的。其中，每行的单词数组会经过过滤并在函数内转化为tuple。如此例子中，flatMap()才是被用来模拟Mapper的，而不是map()。 </p>
<h2 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey()"></a>groupByKey()</h2><p>像之前那样，写一个Reducer来将每个单词的计数求和是挺简单的。而在spark中，reduceByKey()也可以用来求和每个单次的计数。但假如某些情况下输出必须包含所有转为大写后的单词以及它的计数，又该如何实现呢？在MapReduce，如下： </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountUppercaseReducer</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text word, Iterable&lt;IntWritable&gt; counts, Context context)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (IntWritable count : counts) &#123;</span><br><span class="line">      sum += count.get();</span><br><span class="line">    &#125;</span><br><span class="line">    context.write(<span class="keyword">new</span> Text(word.toString().toUpperCase()), <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 但由于在Spark中reduceByKey()自身维持着原始的key，所以并不能这么干。故需要在Spark中模拟这种更切合Reduce API的写法。回想一下，Reducer的reduce()方法接收一个key和values的迭代器，然后吐出一些转换后的东西。groupByKey()再加一个map()能做到：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">... .groupByKey().map &#123; <span class="keyword">case</span> (word,ones) =&gt; (word.toUpperCase, ones.sum) &#125;</span><br></pre></td></tr></table></figure>

<p>groupByKey()仅仅收集所有同一个key的所有value，并不会应用一个reduce函数。这样，任意的转换都能应用于这个key和values的迭代。在这个例子中，key被转成大写，value被直接求和。 </p>
<p>注意！groupByKey()虽然行得通，但它在内存中收集了同一个key所有的value。假如一个key关联着大量的value，worker就会OOM了。尽管这个最直接的模拟Reducer的方案，却在很多时候并非最佳选择。比如说，Spark能在调用reduceByKey后很容易地将key转换成需要的： </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">... .reduceByKey(_ + _).map &#123; <span class="keyword">case</span> (word,total) =&gt; (word.toUpperCase,total) &#125;</span><br></pre></td></tr></table></figure>

<p>最好让Spark来管理reduce而非收集起来再手动求和。 </p>
<h2 id="setup-和cleanup"><a href="#setup-和cleanup" class="headerlink" title="setup()和cleanup()"></a>setup()和cleanup()</h2><p>在MapReduce，一个Mapper和Reducer可以声明一个setup()方法，在所有输入被处理之前调用，也许用来分配想数据库连接等昂贵的资源，之后一个cleanup()方法来释放这些资源： </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SetupCleanupMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Connection dbConnection;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">    dbConnection = ...;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">    dbConnection.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> Spark的map()和flatMap()方法每次仅操作输入的一条记录，而且没有提供任何方法来在批量处理value的前后执行一些代码。似乎可以简单地将setup和cleanup代码放在Spark的map()调用前后：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dbConnection = ...</span><br><span class="line">lines.map(... dbConnection.createStatement(...) ...)</span><br><span class="line">dbConnection.close() <span class="comment">// Wrong!</span></span><br></pre></td></tr></table></figure>

<p>因而，有如下几个愿意你导致失败： </p>
<ul>
<li>它将dbConnection对象放进了map函数闭包中，那就需要这个对象是可序列化的（比如实现java.io.Serializable）。通常一个数据库连接不是可序列化的。 </li>
<li>map()是一个转换(transformation)而非操作(operation)，它是延迟执行的(lazily evaluated)。这里连接就不能马上被关闭。 </li>
<li>即使这样，连接只会在driver端关闭，对序列化的副本却没有释放掉资源。 </li>
</ul>
<p>事实上，Spark的map()和flatMap()都不是最近似Mapper的，而是mapPartitions()方法。这个方法不是将一个值映射为另一个值，而是将一个values的迭代器 </p>
<p>映射为另一个values的迭代器。就像一个“块映射”(bulk map)的方法。这就是说mapPatitions()方法能在它开始的时候在本地分配资源，并在完成所有values的映射后释放他们。 </p>
<p>增加setup代码很容易，增加cleanup代码稍微难一些，因为找出什么时候迭代器转换完全执行完还不是那么容易。比如：这么写不行： </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lines.mapPartitions &#123; valueIterator =&gt;</span><br><span class="line">  <span class="keyword">val</span> dbConnection = ... <span class="comment">// OK</span></span><br><span class="line">  <span class="keyword">val</span> transformedIterator = valueIterator.map(... dbConnection ...)</span><br><span class="line">  dbConnection.close() <span class="comment">// Still wrong! May not have evaluated iterator</span></span><br><span class="line">  transformedIterator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 一个更完善的构想大概如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">lines.mapPartitions &#123; valueIterator =&gt;</span><br><span class="line">  <span class="keyword">if</span> (valueIterator.isEmpty) &#123;</span><br><span class="line">    <span class="type">Iterator</span>[...]()</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> dbConnection = ...</span><br><span class="line">    valueIterator.map &#123; item =&gt;</span><br><span class="line">      <span class="keyword">val</span> transformedItem = ...</span><br><span class="line">      <span class="keyword">if</span> (!valueIterator.hasNext) &#123;</span><br><span class="line">        dbConnection.close()</span><br><span class="line">      &#125;</span><br><span class="line">      transformedItem</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>尽管毫无疑问没之前写的简洁，但是总算完成了。 </p>
<p>没有flatMapPartitions()方法。但先调用mapPartitions()再接一个flatMap(a=&gt;a)也能达到相同的效果。 </p>
<p>同理，一个带有setup()和cleanup()的Reducer等价于一个groupByKey()再接一个mapPartitions()调用。不过还是得注意上面 关于使用groupBykey()的忠告。 </p>
<h2 id="等一下，还有更多"><a href="#等一下，还有更多" class="headerlink" title="等一下，还有更多"></a>等一下，还有更多</h2><p>MapReduce开发者会指出，还有更多的API没有提及到的： </p>
<ul>
<li>MapReduce支持一种特殊的Reducer，称为Combiner，能减少从Mapper输出的shffle数据的大小。 </li>
<li>通过Partitioner，支持自定义分区，以及通过grouping Comparator实现Reducer的自定义分组。 </li>
<li>Context对象提供了Counter API这种方式来累计统计。 </li>
<li>Reducer的生命周期内看到的keys总是有序的。 </li>
<li>MapReduce有自己的 可写的序列化数组结构(Wriable serialization scheme)。 </li>
<li>Mapper和Reducer能一次吐出多个输出(multiple outputs)。 </li>
<li>MapReduce本身有很多调优参数。 </li>
</ul>
<p>Spark有很多途径来实现或者绕过这些概念，使用API如Accumulator，partitioner参数以及各种各样如groupBy() 这类函数，Java或者Kryo序列化，缓存，还有其它的方式。为了保持文章精简，剩余的留待下一篇文章。 </p>
<p>MapReduce的概念一直都是有用的。只是现在有一个不同的可能更强大的Hadoop上的实现，以及更 匹配函数式根源的一个函数式语言（译者注：应指的是scala对比java）。理解Spark RDD API与原始Mapper和Reducer API的区别，帮助开发者更好地理解它们真正的工作原理 以及 怎样与之对应地使用Spark惊醒最佳实践。 </p>
<p>第二部分将讲述聚合函数，计数器，分组和序列化。 </p>
<p>原文链接：<a href="https://blog.cloudera.com/blog/2014/09/how-to-translate-from-mapreduce-to-apache-spark/" target="_blank" rel="noopener">https://blog.cloudera.com/blog/2014/09/how-to-translate-from-mapreduce-to-apache-spark/</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">AlexanderKwong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://alexanderkwong.github.io/2019/10/23/%E5%A6%82%E4%BD%95%E5%B0%86MapReduce%E7%BF%BB%E8%AF%91%E4%B8%BASpark-1/">https://alexanderkwong.github.io/2019/10/23/%E5%A6%82%E4%BD%95%E5%B0%86MapReduce%E7%BF%BB%E8%AF%91%E4%B8%BASpark-1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%BF%BB%E8%AF%91/">翻译</a><a class="post-meta__tags" href="/tags/spark/">spark</a><a class="post-meta__tags" href="/tags/mapreduce/">mapreduce</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/10/23/%E5%A6%82%E4%BD%95%E5%B0%86MapReduce%E7%BF%BB%E8%AF%91%E4%B8%BASpark-2/"><i class="fa fa-chevron-left">  </i><span>如何将MapReduce翻译为Spark(2)</span></a></div><div class="next-post pull-right"><a href="/2019/10/23/%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2SparkSql%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8Catalyst/"><span>深入探索SparkSql的优化器Catalyst</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://api.neweb.top/bing.php)"><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By AlexanderKwong</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>